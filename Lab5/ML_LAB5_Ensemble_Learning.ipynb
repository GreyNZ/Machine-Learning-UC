{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_LAB5_Ensemble_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU0MGWJBikJ1"
      },
      "source": [
        "# Assignment 5 - Ensemble Learning\n",
        "\n",
        "We have seen how to create simple classifiers. But some problems are beyond the abilities of these models, so we have two options:\n",
        "\n",
        "Build more complicated learners using sophisticated methods.\n",
        "Use more simple classifiers.\n",
        "We are going to opt for the second approach.\n",
        "\n",
        "For this assignment, we want to create ensemble learners, which are based on the simple models we have seen so far. Much like the \"wisdom of the crowds\", we wish to exploit the wisdom of multiple classifiers to make better predictions than they would individually.\n",
        "\n",
        "We will explore two main classes of ensemble learners: bagging and boosting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uumPZ2dI5wFo"
      },
      "source": [
        "import hashlib\n",
        "import numpy as np\n",
        "\n",
        "def pseudo_random_x(seed=0xDEADBEEF):\n",
        "    \"\"\"Generate an infinite stream of pseudo-random numbers\"\"\"\n",
        "    state = (0xffffffff & seed)/0xffffffff\n",
        "    while True:\n",
        "        h = hashlib.sha256()\n",
        "        h.update(bytes(str(state), encoding='utf8'))\n",
        "        bits = int.from_bytes(h.digest()[-8:], 'big')\n",
        "        state = bits >> 32\n",
        "        r = (0xffffffff & bits)/0xffffffff\n",
        "        yield r\n",
        "\n",
        "\n",
        "def bootstrap_x(dataset, sample_size):\n",
        "    rand = pseudo_random()\n",
        "    while True:\n",
        "        result = []\n",
        "        for i in range(sample_size):\n",
        "            r = next(rand)\n",
        "            index = int(r * len(dataset))\n",
        "            result.append(dataset[index])\n",
        "        yield np.array(result)\n",
        "\n",
        "\n",
        "def voting_ensemble_x(classifiers):\n",
        "    def f(point):\n",
        "        d = {}\n",
        "        for c in classifiers:\n",
        "            d[c(point)] = d.get(c(point), 0) + 1\n",
        "        invert = [(y, -x) for x, y in d.items()]\n",
        "        return -max(invert)[1]\n",
        "    return f\n",
        "\n",
        "def bagging_model_x(learner, dataset, n_models, sample_size):\n",
        "    type(dataset)\n",
        "    models = []\n",
        "    samples = bootstrap(dataset, sample_size)\n",
        "    for i in range(n_models): # for each of the t iterations\n",
        "        sample = next(samples)  # sample n instances from training set\n",
        "        model = learner(sample)\n",
        "        models.append(model)\n",
        "    return voting_ensemble(models)\n",
        "\n",
        "\n",
        "class weighted_bootstrap:\n",
        "    def init(self, dataset, weights, sample_size):\n",
        "        self.dataset = dataset\n",
        "        self.weights = weights\n",
        "        self.sample_size = sample_size\n",
        "        self.rand = pseudorandom()\n",
        "\n",
        "    def iter(self):\n",
        "        return self\n",
        "\n",
        "    def next(self):\n",
        "        sample = []\n",
        "        for  in range(self.sample_size):\n",
        "            index = self.weighted_sample()\n",
        "            sample.append(self.dataset[index])\n",
        "        return np.array(sample)\n",
        "\n",
        "    def weighted_sample(self):\n",
        "        running_sum = list(accumulate(self.weights))\n",
        "        total = running_sum[-1]\n",
        "        r = next(self.rand) * total\n",
        "        index = bisect_right(running_sum, r)\n",
        "        return index\n",
        "\n",
        "\n",
        "\n",
        "############### Q5\n",
        "\n",
        "import hashlib\n",
        "from itertools import accumulate\n",
        "from bisect import bisect_right\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def pseudo_random(seed=0xDEADBEEF):\n",
        "    \"\"\"Generate an infinite stream of pseudo-random numbers\"\"\"\n",
        "    state = (0xffffffff & seed) / 0xffffffff\n",
        "    while True:\n",
        "        h = hashlib.sha256()\n",
        "        h.update(bytes(str(state), encoding='utf8'))\n",
        "        bits = int.from_bytes(h.digest()[-8:], 'big')\n",
        "        state = bits >> 32\n",
        "        r = (0xffffffff & bits) / 0xffffffff\n",
        "        yield r\n",
        "\n",
        "\n",
        "class weighted_bootstrap:\n",
        "    def __init__(self, dataset, weights, sample_size):\n",
        "        self.dataset = dataset\n",
        "        self.weights = weights\n",
        "        self.sample_size = sample_size\n",
        "        self.rand = pseudo_random()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        sample = []\n",
        "        for _ in range(self.sample_size):\n",
        "            index = self.weighted_sample()\n",
        "            sample.append(self.dataset[index])\n",
        "        return np.array(sample)\n",
        "\n",
        "    def weighted_sample(self):\n",
        "        running_sum = list(accumulate(self.weights))\n",
        "        total = running_sum[-1]\n",
        "        r = next(self.rand) * total\n",
        "        index = bisect_right(running_sum, r)\n",
        "        return index\n",
        "\n",
        "\n",
        "def adaboost(learner, dataset, n_models):\n",
        "    models = []\n",
        "    weights = [1/dataset.shape[0]] * dataset.shape[0] # assign equal weight to each training instance\n",
        "    classes = set()\n",
        "    samples = weighted_bootstrap(dataset, weights, dataset.shape[0] )\n",
        "    for t in range(n_models): # for t iterations\n",
        "        sample = next(samples) # get next weighted dataset\n",
        "        model = learner(sample)    # Apply learning algorithm to weighted dataset\n",
        "        e = 0\n",
        "        for i in range(len(dataset)):\n",
        "            prediction = int(model(dataset[i][:-1]))\n",
        "            target = int(dataset[i][-1])\n",
        "            misclassified = 1 if target != prediction else 0\n",
        "            e += misclassified * weights[i]\n",
        "        if e == 0 or e >= 0.5:\n",
        "            break\n",
        "        for i in range(len(dataset)):\n",
        "            prediction = int(model(dataset[i][:-1]))\n",
        "            target = int(dataset[i][-1])\n",
        "            classes.add(target)\n",
        "            misclassified = 1 if target != prediction else 0\n",
        "            if misclassified == 0:\n",
        "                weights[i] *= e/(1-e)\n",
        "        models.append((model, e))\n",
        "        weights = [w/sum(weights) for w in weights]\n",
        "        samples.weights = weights\n",
        "\n",
        "    def boosted_model(data, models=models):\n",
        "        num_class = len(classes)\n",
        "        class_weights = [0] * num_class\n",
        "        for model, e in models:\n",
        "            prediction = int(model(data))\n",
        "            if e == 0:\n",
        "                class_weights[prediction] +=  float('inf')\n",
        "            else:\n",
        "                class_weights[prediction] += -1 * math.log(e / (1- e))\n",
        "        return class_weights.index(max(class_weights))\n",
        "\n",
        "    return boosted_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import sklearn.datasets\n",
        "import sklearn.utils\n",
        "import sklearn.linear_model\n",
        "\n",
        "digits = sklearn.datasets.load_digits()\n",
        "data, target = sklearn.utils.shuffle(digits.data, digits.target, random_state=3)\n",
        "train_data, train_target = data[:-5, :], target[:-5]\n",
        "test_data, test_target = data[-5:, :], target[-5:]\n",
        "dataset = np.hstack((train_data, train_target.reshape((-1, 1))))\n",
        "\n",
        "def linear_learner(dataset):\n",
        "    features, target = dataset[:, :-1], dataset[:, -1]\n",
        "    model = sklearn.linear_model.SGDClassifier(random_state=1, max_iter=1000, tol=0.001).fit(features, target)\n",
        "    return lambda v: model.predict(np.array([v]))[0]\n",
        "\n",
        "boosted = adaboost(linear_learner, dataset, 10)\n",
        "for (v, c) in zip(test_data, test_target):\n",
        "    print(int(boosted(v)), c)\n",
        "\n",
        "import sklearn.datasets\n",
        "import sklearn.utils\n",
        "import sklearn.linear_model\n",
        "\n",
        "iris = sklearn.datasets.load_iris()\n",
        "data, target = sklearn.utils.shuffle(iris.data, iris.target, random_state=0)\n",
        "train_data, train_target = data[:-5, :], target[:-5]\n",
        "test_data, test_target = data[-5:, :], target[-5:]\n",
        "dataset = np.hstack((train_data, train_target.reshape((-1, 1))))\n",
        "\n",
        "def linear_learner(dataset):\n",
        "    features, target = dataset[:, :-1], dataset[:, -1]\n",
        "    model = sklearn.linear_model.SGDClassifier(random_state=1, max_iter=1000, tol=0.001).fit(features, target)\n",
        "    return lambda v: model.predict(np.array([v]))[0]\n",
        "\n",
        "boosted = adaboost(linear_learner, dataset, 10)\n",
        "for (v, c) in zip(test_data, test_target):\n",
        "    print(int(boosted(v)), c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgXfq2Hairhx"
      },
      "source": [
        "## Q1\n",
        "\n",
        "Define a function voting_ensemble(classifiers) that takes as a parameter a list of classifiers, and returns a new classifier that reports, for a given input, the winning vote amongst all the given classifiers on that input.\n",
        "\n",
        "You can assume that the input that is passed onto the output of voting_ensemble can be consumed by all the classifiers in classifiers.\n",
        "\n",
        "In the case of a tie, return the output that sorts lowest (whether this is numeric or lexicographic). You may assume there will be at least one classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgHBa-ekiXaH",
        "outputId": "f0ccea9a-f671-401f-9c6a-6c049935e4df"
      },
      "source": [
        "def voting_ensemble(classifiers):\n",
        "  def f(data):\n",
        "    dicty = {}\n",
        "    for classifier in classifiers:\n",
        "      dicty[classifier(data)] = dicty.get(classifier(data), 0) + 1\n",
        "    return -1 * max([(y, -x) for x, y in dicty.items()])[1]\n",
        "  return f\n",
        "\n",
        "# def voting_ensemble(classifiers):\n",
        "#   def f(data):\n",
        "#     dicty = {}\n",
        "#     for classifier in classifiers:\n",
        "#       dicty[classifier(data)] = dicty.get(classifier(data), 0) + 1\n",
        "#     return max(dicty, key=dicty.get)\n",
        "#   return f\n",
        "\n",
        "# Modelling y > x^2\n",
        "classifiers = [\n",
        "    lambda p: 1 if 1.0 * p[0] < p[1] else 0,\n",
        "    lambda p: 1 if 0.9 * p[0] < p[1] else 0,\n",
        "    lambda p: 1 if 0.8 * p[0] < p[1] else 0,\n",
        "    lambda p: 1 if 0.7 * p[0] < p[1] else 0,\n",
        "    lambda p: 1 if 0.5 * p[0] < p[1] else 0,\n",
        "]\n",
        "data_points = [(0.2, 0.03), (0.1, 0.12), \n",
        "               (0.8, 0.63), (0.9, 0.82)]\n",
        "c = voting_ensemble(classifiers)\n",
        "for v in data_points:\n",
        "    print(c(v))\n",
        "\n",
        "print(\"Result should match 0 1 0 1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "Result should match 0 1 0 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XajnujF_m8Bz",
        "outputId": "70f7573d-25f6-4863-f032-a79b9c8e429c"
      },
      "source": [
        "#hidden test\n",
        "classifiers = [lambda x: 0, lambda x: 1]\n",
        "c1 = voting_ensemble(classifiers)\n",
        "c2 = voting_ensemble(classifiers[::-1])\n",
        "print(c1(\"Hello\"))\n",
        "print(c2(\"Goodbye\"))\n",
        "\n",
        "print(\"Result should match 0 0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "Result should match 0 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDzgQd36mu2w"
      },
      "source": [
        "# Q2\n",
        "\n",
        "\n",
        "Define a function bootstrap(dataset, sample_size) that samples the given dataset to produce samples of sample_size. Your function should be a generator: that is, calling it produces an iterator itr that produces a new sample when next(itr) is called.\n",
        "\n",
        "Datasets are given as numpy arrays, where each row is a single feature vector. You should sample the rows. Use the following algorithm: use our pseudo_random to generate a random number r, convert it to an index by int(r * len(dataset)), then add the row at that index to the sample. Once the sample has sample_size many rows, yield the sample.\n",
        "\n",
        "Note that by using yield your code automatically becomes a generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq0wG3EiVzkL"
      },
      "source": [
        "Python provides a wonderful random number generator in the random module. Unfortunately, even if we seed it, there is no guarantee that we will get the same sequence of random numbers across all versions and operating systems. While it will likely be the same, to make things easier for you (and us), we provide the following code to generate consistent random numbers everywhere. \n",
        "\n",
        "import hashlib\n",
        "def pseudo_random(seed=0xDEADBEEF):\n",
        "    \"\"\"Generate an infinite stream of pseudo-random numbers\"\"\"\n",
        "    state = (0xffffffff & seed)/0xffffffff\n",
        "    while True:\n",
        "        h = hashlib.sha256()\n",
        "        h.update(bytes(str(state), encoding='utf8'))\n",
        "        bits = int.from_bytes(h.digest()[-8:], 'big')\n",
        "        state = bits >> 32\n",
        "        r = (0xffffffff & bits)/0xffffffff\n",
        "        yield r\n",
        "\n",
        "def take(n, iterator):\n",
        "    while n > 0:\n",
        "        yield next(iterator)\n",
        "        n -= 1\n",
        "\n",
        "for i in take(5, pseudo_random()):\n",
        "    print(i)\n",
        "This is not a great random number generator, but it's not as terrible as it first seems: using hashes as a source for random numbers is possible. You are welcome to research this in your own time.\n",
        "\n",
        "We will use this pseudo_random function for most of the remaining questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf5oDMcvmvBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1512f6-3c9c-44f3-e071-cdcf8218585b"
      },
      "source": [
        "import hashlib\n",
        "import numpy as np\n",
        "\n",
        "def pseudo_random(seed=0xDEADBEEF):\n",
        "    \"\"\"Generate an infinite stream of pseudo-random numbers\"\"\"\n",
        "    state = (0xffffffff & seed)/0xffffffff\n",
        "    while True:\n",
        "        h = hashlib.sha256()\n",
        "        h.update(bytes(str(state), encoding='utf8'))\n",
        "        bits = int.from_bytes(h.digest()[-8:], 'big')\n",
        "        state = bits >> 32\n",
        "        r = (0xffffffff & bits)/0xffffffff\n",
        "        yield r\n",
        "\n",
        "def bootstrap(dataset, sample_size):\n",
        "  randy = pseudo_random()\n",
        "  while randy:\n",
        "    results = []\n",
        "    for _ in range(sample_size):\n",
        "      results.append(dataset[int(next(randy) * len(dataset))])\n",
        "    yield np.array(results)\n",
        "\n",
        "\n",
        "dataset = np.array([[1, 0, 2, 3],\n",
        "                    [2, 3, 0, 0],\n",
        "                    [4, 1, 2, 0],\n",
        "                    [3, 2, 1, 0]])\n",
        "ds_gen = bootstrap(dataset, 3)\n",
        "print(next(ds_gen))\n",
        "print(next(ds_gen))\n",
        "\n",
        "print(f\"match [[4 1 2 0] [2 3 0 0] [1 0 2 3]][[3 2 1 0] [2 3 0 0] [1 0 2 3]]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4 1 2 0]\n",
            " [2 3 0 0]\n",
            " [1 0 2 3]]\n",
            "[[3 2 1 0]\n",
            " [2 3 0 0]\n",
            " [1 0 2 3]]\n",
            "match [[4 1 2 0] [2 3 0 0] [1 0 2 3]][[3 2 1 0] [2 3 0 0] [1 0 2 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F24O_veUmvUY"
      },
      "source": [
        "# Q3\n",
        "\n",
        "\n",
        "Define a function bagging_model(learner, dataset, n_models, sample_size) that returns a new model based on the learner, but replicated n_models times over the bootstrapped dataset of size sample_size.\n",
        "\n",
        "A model is a function that takes a feature vector (a d length numpy array) and produces a prediction (value).\n",
        "\n",
        "A learner is a function that takes a dataset and produces a model.\n",
        "\n",
        "The dataset is a n×(d+1) numpy array where each row is a new feature vector; the final column is the class. \n",
        "\n",
        "Use your voting_ensemble and bootstrap functions. Don't forget to include them, along with their dependencies – including the random number generator!\n",
        "\n",
        "Hint: If your function is more than half a dozen lines, you might be going about this the wrong way.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsRKTwrKmvcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0a64e0-68fc-436f-a10d-98307c50f55b"
      },
      "source": [
        "import hashlib\n",
        "import numpy as np\n",
        "\n",
        "def voting_ensemble(classifiers):\n",
        "  def f(data):\n",
        "    dicty = {}\n",
        "    for classifier in classifiers:\n",
        "      dicty[classifier(data)] = dicty.get(classifier(data), 0) + 1\n",
        "    return -1 * max([(y, -x) for x, y in dicty.items()])[1]\n",
        "  return f\n",
        "\n",
        "def pseudo_random(seed=0xDEADBEEF):\n",
        "    \"\"\"Generate an infinite stream of pseudo-random numbers\"\"\"\n",
        "    state = (0xffffffff & seed)/0xffffffff\n",
        "    while True:\n",
        "        h = hashlib.sha256()\n",
        "        h.update(bytes(str(state), encoding='utf8'))\n",
        "        bits = int.from_bytes(h.digest()[-8:], 'big')\n",
        "        state = bits >> 32\n",
        "        r = (0xffffffff & bits)/0xffffffff\n",
        "        yield r\n",
        "\n",
        "def bootstrap(dataset, sample_size):\n",
        "  randy = pseudo_random()\n",
        "  while randy:\n",
        "    results = []\n",
        "    for _ in range(sample_size):\n",
        "      results.append(dataset[int(next(randy) * len(dataset))])\n",
        "    yield np.array(results)\n",
        "\n",
        "def bagging_modelx(learner, dataset, n_models, sample_size):\n",
        "    type(dataset)\n",
        "    models = []\n",
        "    samples = bootstrap(dataset, sample_size)\n",
        "    for i in range(n_models): # for each of the t iterations\n",
        "        sample = next(samples)  # sample n instances from training set\n",
        "        model = learner(sample)\n",
        "        models.append(model)\n",
        "    return voting_ensemble(models)\n",
        "\n",
        "def bagging_model(learner, dataset, n_models, sample_size):\n",
        "  m = []\n",
        "  s = bootstrap(dataset, sample_size)\n",
        "  for _ in range(n_models):\n",
        "    m.append(learner(next(s)))\n",
        "  return voting_ensemble(m)\n",
        "\n",
        "import sklearn.datasets\n",
        "import sklearn.utils\n",
        "import sklearn.tree\n",
        "\n",
        "iris = sklearn.datasets.load_iris()\n",
        "data, target = sklearn.utils.shuffle(iris.data, iris.target, random_state=1)\n",
        "train_data, train_target = data[:-5, :], target[:-5]\n",
        "test_data, test_target = data[-5:, :], target[-5:]\n",
        "dataset = np.hstack((train_data, train_target.reshape((-1, 1))))\n",
        "\n",
        "def tree_learner(dataset):\n",
        "    features, target = dataset[:, :-1], dataset[:, -1]\n",
        "    model = sklearn.tree.DecisionTreeClassifier(random_state=1).fit(features, target)\n",
        "    return lambda v: model.predict(np.array([v]))[0]\n",
        "\n",
        "bagged = bagging_model(tree_learner, dataset, 50, len(dataset)//2)\n",
        "# Note that we get the first one wrong!\n",
        "for (v, c) in zip(test_data, test_target):\n",
        "    print(int(bagged(v)), c)\n",
        "\n",
        "print(\"\\nshould match below\\n1 2\\n2 2\\n1 1 \\n2 2 \\n0 0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2\n",
            "2 2\n",
            "1 1\n",
            "2 2\n",
            "0 0\n",
            "\n",
            "should match below\n",
            "1 2\n",
            "2 2\n",
            "1 1 \n",
            "2 2 \n",
            "0 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bY0lIzemrIa"
      },
      "source": [
        "# Q4\n",
        "\n",
        "Bagging works well, but it doesn't consider how good any of the previous models were at classifying the data. Instead, we can consider how good the existing models are, and specifically train the next models to fill in their shortcomings. This is called boosting.\n",
        "\n",
        "We will implement an algorithm called AdaBoost. This algorithm works by weighting the likelihood of sampling different values from the dataset based on how often they are incorrectly classified. Then, when making a prediction, the final class is no longer just a vote from the models: the error of each model is factored in the vote. Thus we combine a series of models that \"adapt\" to the \"bootstraps\" of the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Define a class weighted_bootstrap that is initialised with a dataset, weights, and a sample_size, where dataset and weights must be the same length. This class should be an iterator (that is, __iter__ should return self), and each call to next (that is, the __next__ method) should produce a new bootstrapped sample of sample_size rows, where each row has a chance of occurring proportional to its weight. \n",
        "\n",
        "The class should allow modification of the weights attribute so that the weights can change dynamically.\n",
        "\n",
        "The algorithm we use to do this weighted sample is as follows:\n",
        "\n",
        "Calculate the running sum of the weights\n",
        "Generate a random value up to the sum of the weights\n",
        "Find the index i of the first value in the running sum to exceed this random value.\n",
        "The row in position i in the dataset gets added to the sample.\n",
        "Repeat until a complete sample is drawn. \n",
        "Notes\n",
        "Don't forget to use our pseudo-random number generator.\n",
        "We are violating the naming convention that classes are CamelCase to make weighted_bootstrap seem like a function, because we want it to be used like a function. This seems a bit icky, but there is precedent... have you ever looked closely at the built-in range \"function\"?\n",
        "The Airfoil Self-Noise Data used in some test cases is available here\n",
        "The running sum of [1, 2, 3] is [1, 3, 6].\n",
        "Pay attention to the sentence \"Generate a random value up to the sum of the weights.\" The sum of weights is not necessarily an integer!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnOm5WOfmrgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d9e8c3-591f-446a-baf5-497a00137aa4"
      },
      "source": [
        "import hashlib\n",
        "from itertools import accumulate\n",
        "from bisect import bisect_right\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def pseudo_random(seed=0xDEADBEEF):\n",
        "    \"\"\"Generate an infinite stream of pseudo-random numbers\"\"\"\n",
        "    state = (0xffffffff & seed) / 0xffffffff\n",
        "    while True:\n",
        "        h = hashlib.sha256()\n",
        "        h.update(bytes(str(state), encoding='utf8'))\n",
        "        bits = int.from_bytes(h.digest()[-8:], 'big')\n",
        "        state = bits >> 32\n",
        "        r = (0xffffffff & bits) / 0xffffffff\n",
        "        yield r\n",
        "\n",
        "\n",
        "class weighted_bootstrap:\n",
        "    def __init__(self, dataset, weights, sample_size):\n",
        "        self.dataset = dataset\n",
        "        self.weights = weights\n",
        "        self.sample_size = sample_size\n",
        "        self.randy = pseudo_random()\n",
        "\n",
        "    def __next__(self):\n",
        "        results = []\n",
        "        for _ in range(self.sample_size):\n",
        "            results.append(self.dataset[self.weighted_sample()])\n",
        "        return np.array(results)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def weighted_sample(self):\n",
        "        running_sum = list(accumulate(self.weights))\n",
        "        result = bisect_right(running_sum, next(self.randy) * running_sum[-1])\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "wbs = weighted_bootstrap([1, 2, 3, 4, 5], [1, 1, 1, 1, 1], 5)\n",
        "sample = next(wbs)\n",
        "print(type(sample))\n",
        "print(sample)\n",
        "\n",
        "print(next(wbs))\n",
        "print()\n",
        "wbs.weights = [1, 1, 1000, 1, 1]\n",
        "print(next(wbs))\n",
        "print(next(wbs))\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\nShould match following:\\n<class 'numpy.ndarray'> \\n[3 2 1 5 3] \\n[1 3 2 1 3] \\n\\n[3 3 3 3 3] \\n[3 3 3 3 3]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[3 2 1 5 3]\n",
            "[1 3 2 1 3]\n",
            "\n",
            "[3 3 3 3 3]\n",
            "[3 3 3 3 3]\n",
            "\n",
            "\n",
            "Should match following:\n",
            "<class 'numpy.ndarray'> \n",
            "[3 2 1 5 3] \n",
            "[1 3 2 1 3] \n",
            "\n",
            "[3 3 3 3 3] \n",
            "[3 3 3 3 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTGJSTSMTIPZ",
        "outputId": "85401bda-68cd-4671-913a-b73162384b30"
      },
      "source": [
        "# hidden test\n",
        "\t\n",
        "wbs = weighted_bootstrap([1, 2, 3, 4, 5], [1, 2, 1, 2, 1], 5)\n",
        "print(next(wbs))\n",
        "print(next(wbs))\n",
        "print()\n",
        "wbs.weights = [5, 2, 1, 4, 2]\n",
        "print(next(wbs))\n",
        "print(next(wbs))\n",
        "print()\n",
        "wbs.weights = [0, 0, 1, 0, 1]\n",
        "print(next(wbs))\n",
        "print(next(wbs))\n",
        "\n",
        "print(\"\\n\\nShould match following:\\n[4 2 1 5 3] \\n[2 3 2 1 3] \\n\\n[5 5 4 1 4]\\n[1 4 1 4 2]\\n\\n[3 5 5 3 3]\\n[5 3 5 3 3]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4 2 1 5 3]\n",
            "[2 3 2 1 3]\n",
            "\n",
            "[5 5 4 1 4]\n",
            "[1 4 1 4 2]\n",
            "\n",
            "[3 5 5 3 3]\n",
            "[5 3 5 3 3]\n",
            "\n",
            "\n",
            "Should match following:\n",
            "[4 2 1 5 3] \n",
            "[2 3 2 1 3] \n",
            "\n",
            "[5 5 4 1 4]\n",
            "[1 4 1 4 2]\n",
            "\n",
            "[3 5 5 3 3]\n",
            "[5 3 5 3 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "301tuaNCoaBX"
      },
      "source": [
        "# Q5\n",
        "\n",
        "\n",
        "Define a function adaboost(learner, dataset, n_models) that builds a boosted ensemble model consisting of n_models simple models made from the learner, which were trained on weighted bootstrapped samples from the dataset.\n",
        "\n",
        "As with bagging, a learner is a function that takes a dataset and returns a model. A model is a function which takes a feature vector and returns a classification.\n",
        "\n",
        "Follow the pseudocode in the lecture notes.\n",
        "\n",
        "Notes\n",
        "Remember to include all the code from your support functions, including the weighted bootstrapping class and the random number generator.\n",
        "Boosting is more sophisticated that bagging, half a dozen lines won't cut it here.\n",
        "To compute the model's error on the dataset, add up the weight of the instances (rows) that are misclassified. This error is used to update the weights. It is also stored with the model so that later, when combining the outputs of the classifiers, it can be used to compute the correct weight for the classifier.\n",
        "When e=0, define the value of log(e/(1-e)) to be -infinity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIzGNogCoaIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9320cd1-2acd-459a-ff63-bf9991ff4df2"
      },
      "source": [
        "import hashlib\n",
        "from itertools import accumulate\n",
        "from bisect import bisect_right\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def pseudo_random(seed=0xDEADBEEF):\n",
        "    \"\"\"Generate an infinite stream of pseudo-random numbers\"\"\"\n",
        "    state = (0xffffffff & seed) / 0xffffffff\n",
        "    while True:\n",
        "        h = hashlib.sha256()\n",
        "        h.update(bytes(str(state), encoding='utf8'))\n",
        "        bits = int.from_bytes(h.digest()[-8:], 'big')\n",
        "        state = bits >> 32\n",
        "        r = (0xffffffff & bits) / 0xffffffff\n",
        "        yield r\n",
        "\n",
        "\n",
        "class weighted_bootstrap:\n",
        "    def __init__(self, dataset, weights, sample_size):\n",
        "        self.dataset = dataset\n",
        "        self.weights = weights\n",
        "        self.sample_size = sample_size\n",
        "        self.randy = pseudo_random()\n",
        "\n",
        "    def __next__(self):\n",
        "        results = []\n",
        "        for _ in range(self.sample_size):\n",
        "            results.append(self.dataset[self.weighted_sample()])\n",
        "        return np.array(results)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def weighted_sample(self):\n",
        "        running_sum = list(accumulate(self.weights))\n",
        "        result = bisect_right(running_sum, next(self.randy) * running_sum[-1])\n",
        "        return result\n",
        "\n",
        "\n",
        "def adaboost(learner, dataset, n_models):\n",
        "    models, class_set = [], set()\n",
        "    weights = [1/dataset.shape[0]] * dataset.shape[0]\n",
        "    w_bs = weighted_bootstrap(dataset, weights, dataset.shape[0])\n",
        "    for t in range(n_models):\n",
        "        e, m = 0, learner(next(w_bs))\n",
        "        for i in range(len(dataset)):\n",
        "            mis_clas = 1 if int(dataset[i][-1]) != int(m(dataset[i][:-1])) else 0\n",
        "            e += mis_clas * weights[i]\n",
        "        if e >= 0.5 or e == 0:\n",
        "            break\n",
        "        for i in range(len(dataset)):\n",
        "            class_set.add(int(dataset[i][-1]))\n",
        "            mis_clas = 1 if int(dataset[i][-1]) != int(m(dataset[i][:-1])) else 0\n",
        "            if mis_clas == 0:\n",
        "                weights[i] *= e/(1-e)\n",
        "        models.append((m, e))\n",
        "        weights = [weight/sum(weights) for weight in weights]\n",
        "        w_bs.weights = weights\n",
        "\n",
        "    def boosted_model(data, models=models):\n",
        "        num_class = len(class_set)\n",
        "        weights = [0] * num_class\n",
        "        for m, e in models:\n",
        "            p = int(m(data))\n",
        "            if e != 0:\n",
        "                weights[p] += -1 * math.log(e / (1- e))\n",
        "            else:\n",
        "                weights[p] = float('inf')\n",
        "        result = weights.index(max(weights))\n",
        "        return result\n",
        "\n",
        "    return boosted_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import sklearn.datasets\n",
        "import sklearn.utils\n",
        "import sklearn.linear_model\n",
        "\n",
        "digits = sklearn.datasets.load_digits()\n",
        "data, target = sklearn.utils.shuffle(digits.data, digits.target, random_state=3)\n",
        "train_data, train_target = data[:-5, :], target[:-5]\n",
        "test_data, test_target = data[-5:, :], target[-5:]\n",
        "dataset = np.hstack((train_data, train_target.reshape((-1, 1))))\n",
        "\n",
        "def linear_learner(dataset):\n",
        "    features, target = dataset[:, :-1], dataset[:, -1]\n",
        "    model = sklearn.linear_model.SGDClassifier(random_state=1, max_iter=1000, tol=0.001).fit(features, target)\n",
        "    return lambda v: model.predict(np.array([v]))[0]\n",
        "\n",
        "boosted = adaboost(linear_learner, dataset, 10)\n",
        "for (v, c) in zip(test_data, test_target):\n",
        "    print(int(boosted(v)), c)\n",
        "\n",
        "print(\"\\n\\nShould match: \\n6 6\\n1 1\\n0 0\\n2 2\\n1 1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 6\n",
            "1 1\n",
            "0 0\n",
            "2 2\n",
            "1 1\n",
            "\n",
            "\n",
            "Should match: \n",
            "6 6\n",
            "1 1\n",
            "0 0\n",
            "2 2\n",
            "1 1\n"
          ]
        }
      ]
    }
  ]
}